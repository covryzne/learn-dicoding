# -*- coding: utf-8 -*-
"""Netflix Recommender System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VAjrJSZprcKtiugJ5lQGPZ3t1Faxq2R2

# **Mount to Drive**
"""

from google.colab import drive
drive.mount('/content/drive')

"""# **Import Library**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score
from sklearn.metrics import mean_squared_error

"""# **Load Dataset**"""

df = pd.read_csv('/content/drive/MyDrive/Kuliah/Dicoding/Datasets/netflix_titles.csv')

df.info()

df['date_added'] = pd.to_datetime(df['date_added'], errors= 'coerce')
df['date_added'].dtype

df.info()

df.head()

"""# **Cek Missing Values**"""

import pandas as pd

# Cek missing values
missing_values = df.isnull().sum()
missing_percentage = (missing_values / len(df)) * 100
missing_data = pd.DataFrame({
    'Missing Values': missing_values,
    'Percentage': missing_percentage
})

missing_data

"""Terlihat bahwa persentasi missing values yang banyak pada `director` yaitu hampir 30% dan perlu dilakukan handling untuk masing-masing missing values tersebut

# **Handling Missing Values**
"""

# Cek missing values
missing_values = df.isnull().sum()

# Imputasi missing values
df['director'] = df['director'].fillna('Unknown')
df['cast'] = df['cast'].fillna('Unknown')
df['country'] = df['country'].fillna('Unknown')
df['release_year'] = df['release_year'].fillna(df['release_year'].mean())
df['rating'] = df['rating'].fillna(df['rating'].mode()[0])
df['listed_in'] = df['listed_in'].fillna(df['listed_in'].mode()[0])
df['date_added'] = df['date_added'].fillna('Unknown')
df['duration'] = df['duration'].fillna('Unknown')

"""# **Cek Missing Values Setelah Dilakukan Handing**"""

missing_values_after = df.isnull().sum()
missing_values_after

df.head()

"""# **Exploratory Data Analysis**"""

plt.figure(figsize=(10, 6))
sns.countplot(x='rating', order = df['rating'].value_counts().index, data = df)
plt.title('Distribution of content ratings on Netflix')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.xticks(rotation = 45)
plt.show()

# Convert 'date_added' column to datetime, handling errors
df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce')

# Now you can extract the month
df['month_added'] = df['date_added'].dt.month

# Extract the year
df['year_added'] = df['date_added'].dt.year  # Extract the year from 'date_added' column

df['year_added'] = df['date_added'].dt.year
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='year_added', hue='type', order=sorted(df['year_added'].dropna().unique()))
plt.title('Number of TV Shows and Movies Added to Netflix Over the Years')
plt.xlabel('Year Added')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

df.info()

df.head()

top_countries = df['country'].value_counts().head(10)
plt.figure(figsize=(12, 8))
sns.barplot(x=top_countries.values, y=top_countries.index)
plt.title('Top 10 Countries Producing Netflix Content')
plt.xlabel('Number of Titles')
plt.ylabel('Country')
plt.show()

genres = df['listed_in'].str.split(', ', expand=True).stack().value_counts()
plt.figure(figsize=(12, 8))
sns.barplot(x=genres.values[:10], y=genres.index[:10])
plt.title('Top 10 Genres on Netflix')
plt.xlabel('Number of Titles')
plt.ylabel('Genre')
plt.show()

movies = df[df['type'] == 'Movie']
# Replace 'Unknown' with NaN before converting to int
movies['duration'] = pd.to_numeric(movies['duration'].str.replace(' min', ''), errors='coerce')
plt.figure(figsize=(10, 6))
sns.histplot(movies['duration'], bins=30, kde=True)
plt.title('Distribution of Movie Durations on Netflix')
plt.xlabel('Duration (minutes)')
plt.ylabel('Frequency')
plt.show()

import pandas as pd

# Continue with your plotting code
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='month_added', hue='year_added')
plt.title('Trends in Content Production by Month and Year on Netflix')
plt.xlabel('Month Added')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='release_year', order=df['release_year'].value_counts().index[:10])
plt.title('Most Common Release Years for Content on Netflix')
plt.xlabel('Release Year')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='rating', hue='type', order=df['rating'].value_counts().index)
plt.title('Distribution of Content Ratings by Content Type on Netflix')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

"""# **Menggunakan Model Development Content Based Filtering**

# **Feature Engineering**
Setelah menangani missing values, kita akan mengubah kolom yang berisi teks (seperti `description`, `listed_in`, dll.) menjadi fitur numerik menggunakan TF-IDF.
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd

# Gabungkan beberapa fitur teks untuk representasi konten
df['content'] = df['cast'] + " " + df['director'] + " " + df['listed_in'] + " " + df['description']

# Menggunakan TfidfVectorizer untuk mengubah teks menjadi vektor numerik
tfidf_vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf_vectorizer.fit_transform(df['content'])

# Cek hasil vektorisasi
print(f"TF-IDF Matrix Shape: {tfidf_matrix.shape}")

# Jika ingin melihat matriks secara keseluruhan dalam bentuk dense (penuh), gunakan todense
# Ini akan mengonversi matriks sparse menjadi dense yang bisa dipahami dengan mudah
tfidf_matrix.todense()

# Membuat DataFrame dari matriks TF-IDF yang sudah diubah menjadi dense
pd.DataFrame(
    tfidf_matrix.todense(),  # Konversi matriks sparse menjadi dense
    columns=tfidf_vectorizer.get_feature_names_out(),  # Nama fitur yang dihasilkan oleh TfidfVectorizer
    index=df['title']  # Indeks berdasarkan judul film
)

# Menghitung cosine similarity pada matriks TF-IDF
cosine_sim = cosine_similarity(tfidf_matrix)

# Membuat DataFrame dari variabel cosine_sim dengan baris dan kolom berupa judul film
cosine_sim_df = pd.DataFrame(cosine_sim, index=df['title'], columns=df['title'])
print(f"Cosine Similarity Matrix Shape: {cosine_sim_df.shape}")

# Melihat similarity matrix untuk beberapa film secara acak
print("Sample Cosine Similarity Matrix:")
print(cosine_sim_df.sample(5, axis=1).sample(10, axis=0))  # Menampilkan beberapa baris dan kolom acak

"""# **Sistem Rekomendasi**"""

# Menghitung cosine similarity antara semua film
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

# Membuat fungsi untuk mendapatkan rekomendasi
def get_recommendations(title, cosine_sim=cosine_sim):
    # Menemukan indeks film berdasarkan judul
    idx = df.index[df['title'] == title].tolist()[0]

    # Mendapatkan skor similaritas untuk semua film berdasarkan indeks
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Urutkan film berdasarkan skor similarity (descending)
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Ambil 10 film teratas yang mirip, selain film itu sendiri
    sim_scores = sim_scores[1:11]

    # Ambil indeks film
    movie_indices = [i[0] for i in sim_scores]

    # Return 10 rekomendasi film teratas
    return df['title'].iloc[movie_indices]

# Cek rekomendasi berdasarkan judul film
print(get_recommendations('Breaking Bad'))

title = input("Masukkan judul film: ")
recommendations = get_recommendations(title)
print(recommendations)

"""# **Menggunakan Model Development Collaborative Filtering**"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Embedding, Flatten, Input, Dense
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

df2 = pd.read_csv('/content/drive/MyDrive/Kuliah/Dicoding/Datasets/netflix_titles.csv')

df2.info()

import pandas as pd

# Cek missing values
missing_values = df2.isnull().sum()
missing_percentage = (missing_values / len(df2)) * 100
missing_data = pd.DataFrame({
    'Missing Values': missing_values,
    'Percentage': missing_percentage
})

missing_data

# Cek missing values
missing_values = df2.isnull().sum()
missing_values

# Imputasi missing values
df2['director'] = df2['director'].fillna('Unknown')
df2['cast'] = df2['cast'].fillna('Unknown')
df2['country'] = df2['country'].fillna('Unknown')
df2['release_year'] = df2['release_year'].fillna(df2['release_year'].mean())
df2['rating'] = df2['rating'].fillna(df2['rating'].mode()[0])
df2['listed_in'] = df2['listed_in'].fillna(df2['listed_in'].mode()[0])
df2['date_added'] = df2['date_added'].fillna('Unknown')
df2['duration'] = df2['duration'].fillna('Unknown')

missing_values

# Membuat userId secara acak (misalnya ada 1000 pengguna)
df2['userId'] = np.random.randint(1, 1001, df2.shape[0])

# Menggunakan show_id sebagai movieId
df2['movieId'] = df2['show_id'].astype('category').cat.codes  # Mengubah show_id menjadi movieId unik

# Menangani missing values untuk rating (misalnya impute dengan rating mode)
df2['rating'] = df2['rating'].fillna('Unrated')  # Jika rating missing, beri label 'Unrated'
df2 = df2[df2['rating'] != 'Unrated']  # Menghapus film yang tidak memiliki rating

# Encode rating ke angka untuk model
rating_mapping = {'Unrated': 0, 'G': 1, 'PG': 2, 'PG-13': 3, 'R': 4, 'NC-17': 5}
df2.loc[:, 'rating'] = df2['rating'].map(rating_mapping).fillna(0).astype(int)  # Gunakan .loc untuk perubahan yang aman

# Pilih kolom yang diperlukan untuk collaborative filtering
df_collab = df2[['userId', 'movieId', 'rating']]

# Cek data
df_collab.head()

# Split data menjadi train dan test
train_data, test_data = train_test_split(df_collab, test_size=0.2, random_state=42)

# Menyiapkan input data untuk model
train_user_input = train_data['userId'].values
train_item_input = train_data['movieId'].values
train_ratings = train_data['rating'].values

test_user_input = test_data['userId'].values
test_item_input = test_data['movieId'].values
test_ratings = test_data['rating'].values

"""# **Neural Collaborative Filtering (NCF)**
Pendekatan dalam sistem rekomendasi berbasis deep learning. Model ini memanfaatkan embedding untuk merepresentasikan pengguna dan item dalam ruang vektor berdimensi rendah, dan kemudian menggabungkannya untuk memprediksi interaksi (rating) antara pengguna dan item.
"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Flatten, Dense
from tensorflow.keras.optimizers import Adam
import numpy as np
import tensorflow as tf # Import tensorflow
from tensorflow.keras.metrics import MeanSquaredError
import tensorflow.keras.backend as K # Import Keras backend as K

# Tentukan jumlah pengguna dan item unik
num_users, num_items = df_collab['userId'].nunique(), df_collab['movieId'].nunique()

# Hyperparameters
embedding_dim = 50

# Membatasi userId dan movieId sesuai rentangnya
train_user_input = np.clip(train_user_input, 0, num_users - 1)
train_item_input = np.clip(train_item_input, 0, num_items - 1)

# Cek dimensi dan validasi
assert train_user_input.min() >= 0 and train_user_input.max() <= num_users - 1, "User ID tidak valid!"
assert train_item_input.min() >= 0 and train_item_input.max() <= num_items - 1, "Movie ID tidak valid!"
train_user_input = train_user_input.flatten()
train_item_input = train_item_input.flatten()

# Membuat input layer untuk user dan item
user_input = Input(shape=(1,), dtype=tf.int32)
item_input = Input(shape=(1,), dtype=tf.int32)

# Embedding layer untuk user dan item
user_embedding = Embedding(input_dim=num_users, output_dim=embedding_dim)(user_input)
item_embedding = Embedding(input_dim=num_items, output_dim=embedding_dim)(item_input)

# Flattening embedding layer untuk dihubungkan ke dense layer
user_vec = Flatten()(user_embedding)
item_vec = Flatten()(item_embedding)

# Menggabungkan user dan item embedding
merged = tf.keras.layers.concatenate([user_vec, item_vec])

# Fully connected layer untuk menghasilkan prediksi rating
x = Dense(128, activation='relu')(merged)
x = Dense(64, activation='relu')(x)
output = Dense(1)(x)

# Fungsi untuk menghitung RMSE
def rmse(y_true, y_pred):
    return K.sqrt(K.mean(K.square(y_pred - y_true))) # Now K is defined

# Membuat dan kompilasi model
model = Model(inputs=[user_input, item_input], outputs=output)

# Compile model dengan MSE dan RMSE sebagai metrik
model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=[MeanSquaredError(), rmse])

# Menampilkan summary model
model.summary()

# Pastikan train_ratings memiliki tipe data yang sesuai
train_ratings = train_ratings.astype(np.float32)

# Melatih model
history = model.fit(
    [train_user_input, train_item_input],  # dua input
    train_ratings,  # target ratings
    epochs=30,
    batch_size=64
)

import matplotlib.pyplot as plt

# Mengambil data MSE dan RMSE dari history
mse = history.history['mean_squared_error']
rmse = history.history['rmse']

# Membuat plot MSE selama training
plt.figure(figsize=(12, 6))

# Plot MSE
plt.subplot(1, 2, 1)
plt.plot(mse)
plt.title('Model Mean Squared Error (MSE) During Training')
plt.xlabel('Epoch')
plt.ylabel('MSE')

# Plot RMSE
plt.subplot(1, 2, 2)
plt.plot(rmse)
plt.title('Model Root Mean Squared Error (RMSE) During Training')
plt.xlabel('Epoch')
plt.ylabel('RMSE')

plt.tight_layout()
plt.show()

import numpy as np

# Fungsi untuk mendapatkan rekomendasi film berdasarkan prediksi rating
def get_movie_recommendations(user_id, num_recommendations=10):
    # Menghasilkan semua pasangan (user, movieId) yang mungkin
    movie_ids = np.arange(num_items)  # Semua movieId
    user_ids = np.full_like(movie_ids, user_id)  # Menambahkan user_id yang sama untuk setiap movieId

    # Memprediksi rating untuk semua pasangan (user_id, movie_id)
    predicted_ratings = model.predict([user_ids, movie_ids], verbose=0)

    # Mengurutkan hasil prediksi berdasarkan rating yang tertinggi
    recommended_movie_ids = movie_ids[np.argsort(predicted_ratings.flatten())[::-1]]

    # Ambil beberapa film teratas
    recommended_movie_ids = recommended_movie_ids[:num_recommendations]

    return recommended_movie_ids

# Contoh: Mendapatkan 10 rekomendasi film untuk user dengan ID tertentu
user_id = 70  # Ganti dengan user ID yang ingin direkomendasikan
recommended_movies = get_movie_recommendations(user_id, num_recommendations=10)

# Menampilkan rekomendasi film beserta judulnya
print(f"Top 10 rekomendasi film untuk user {user_id}:")
for movie_id in recommended_movies:
    # Mendapatkan judul film berdasarkan movie_id dari dictionary
    title = df2[df2['movieId'] == movie_id]['title'].values[0]
    print(f"Movie ID: {movie_id}, Title: {title}")