# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/144w6XA95RdTScjPMixlgSKNDkZmIGR18

# **Latihan Membuat Machine Learning Pipeline**

Run di terminal:
1. conda create --name mlops-tfx python==3.9.15
2. conda activate mlops-tfx
3. conda info --envs
4. run python -m pip install -r requirements.txt

## **Import Library**
"""

import tensorflow as tf
from tfx.components import CsvExampleGen, StatisticsGen, SchemaGen, ExampleValidator, Transform, Trainer, Tuner
from tfx.proto import example_gen_pb2
from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext
import os

"""## **Set Variable**"""

PIPELINE_NAME = "sarcasm-pipeline"
SCHEMA_PIPELINE_NAME = "sarcasm-tfdv-schema"

#Directory untuk menyimpan artifact yang akan dihasilkan
PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)

# Path to a SQLite DB file to use as an MLMD storage.
METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')

# Output directory where created models from the pipeline will be exported.
SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)

# from absl import logging
# logging.set_verbosity(logging.INFO)

DATA_ROOT = "data"

interactive_context = InteractiveContext(pipeline_root=PIPELINE_ROOT)

"""## **Data Ingestion**
Melakukan data ingestion menggunakan komponen CsvExampleGen() yang telah disediakan oleh TFX. Komponen ini dipilih karena dataset yang digunakan memiliki format CSV.
"""

output = example_gen_pb2.Output(
    split_config = example_gen_pb2.SplitConfig(splits=[
        example_gen_pb2.SplitConfig.Split(name="train", hash_buckets=8),
        example_gen_pb2.SplitConfig.Split(name="eval", hash_buckets=2)
    ])
)
example_gen = CsvExampleGen(input_base=DATA_ROOT, output_config=output)

"""Membagi dataset yang akan digunakan untuk proses training (“train”) dan evaluasi (“eval”). Pembagian ini dilakukan dengan mengatur parameter output_config dengan rasio 8:2. Rasio ini dapat kita atur menggunakan parameter hash_buckets.

Jika ingin melihat komponen ExampleGen secara interaktif, Anda dapat menjalankan komponen tersebut menggunakan object InteractiveContext() yang telah kita definisikan sebelumnya.
"""

interactive_context.run(example_gen)

"""## **Data Validation**

### **Summary Statistic**

Membuat summary statistic menggunakan komponen StatisticsGen(). Komponen memiliki parameter input berupa examples untuk menerima dataset dari komponen ExampleGen.
"""

statistics_gen = StatisticsGen(
    examples=example_gen.outputs["examples"]
)


interactive_context.run(statistics_gen)

"""Menampilkan summary statistic yang telah dibuat."""

interactive_context.show(statistics_gen.outputs["statistics"])

"""### **Data Schema**

Setelah membuat summary statistic, kita perlu membuat data schema. Pada latihan ini, kita menggunakan komponen ShcemaGen() untuk membuat data schema. Komponen ini akan menerima input berupa summary statistic.
"""

schema_gen = SchemaGen(    statistics=statistics_gen.outputs["statistics"]
)
interactive_context.run(schema_gen)

"""Menampilkan data schema yang telah dibuat."""

interactive_context.show(schema_gen.outputs["schema"])

"""Berdasarkan tampilan data schema di atas, diketahui bahwa dataset yang kita gunakan terdiri dari dua fitur, yaitu “headline” dan “is_sarcastic”. Fitur pertama memiliki tipe bytes, sedangkan fitur kedua memiliki memiliki tipe integer. Selain itu, berdasarkan data schema di atas kedua fitur tersebut haruslah lengkap untuk setiap dataset.

### **Anomali Detect**

Menggunakan komponen ExampleValidator() yang telah disediakan oleh TFX. Komponen ini membutuhkan keluaran dari statistics_gen dan schema_gen sebagai parameter input.
"""

example_validator = ExampleValidator(
    statistics=statistics_gen.outputs['statistics'],
    schema=schema_gen.outputs['schema']
)
interactive_context.run(example_validator)

"""Untuk menampilkan hasil dari validasi dari komponen tersebut, kita bisa menjalankan kode berikut."""

interactive_context.show(example_validator.outputs['anomalies'])

"""## **Data Preprocessing**

Mengubah data mentah menjadi data yang siap digunakan untuk melatih model. Pada latihan ini, kita menggunakan TFT dan komponen Transform untuk melakukan data preprocessing. <br> Ketika melakukan data preprocessing, Anda sangat disarankan untuk menempatkan preprocessing function ke dalam sebuah module tersendiri. Hal ini dilakukan karena komponen Transform akan menerima inputan berupa module file yang berisi preprocessing function.
"""

TRANSFORM_MODULE_FILE = "sarcasm_transform.py"

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {TRANSFORM_MODULE_FILE}
# import tensorflow as tf
# LABEL_KEY = "is_sarcastic"
# FEATURE_KEY = "headline"
# def transformed_name(key):
#     """Renaming transformed features"""
#     return key + "_xf"
# def preprocessing_fn(inputs):
#     """
#     Preprocess input features into transformed features
# 
#     Args:
#         inputs: map from feature keys to raw features.
# 
#     Return:
#         outputs: map from feature keys to transformed features.
#     """
# 
#     outputs = {}
# 
#     outputs[transformed_name(FEATURE_KEY)] = tf.strings.lower(inputs[FEATURE_KEY])
# 
#     outputs[transformed_name(LABEL_KEY)] = tf.cast(inputs[LABEL_KEY], tf.int64)
# 
#     return outputs

transform  = Transform(
    examples=example_gen.outputs['examples'],
    schema= schema_gen.outputs['schema'],
    module_file=os.path.abspath(TRANSFORM_MODULE_FILE)
)
interactive_context.run(transform)

"""## **Model Development**
### Membuat dan Melatih Model Menggunakan Komponen Trainer.
Seperti yang sudah dijelaskan pada materi sebelumnya, komponen Trainer akan membutuhkan beberapa inputan salah satunya adalah sebuah module file yang berisi training function. Pada latihan ini, kita akan membuat sebuah module file berisi beberapa fungsi berikut.

#### **Fungsi transformed_name()**
Fungsi ini digunakan untuk mengubah nama fitur yang telah melalui proses transform.
"""

def transformed_name(key):
    """Renaming transformed features"""
    return key + "_xf"

"""### **Fungsi gzip_reader_fn()**
Fungsi yang digunakan untuk memuat data dalam format TFRecord.
"""

def gzip_reader_fn(filenames):
    """Loads compressed data"""
    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')

"""#### **Fungsi input_fn()**
Memuat transformed_feature yang dihasilkan oleh komponen Transform dan membaginya ke dalam beberapa batch.
"""

def input_fn(file_pattern,
             tf_transform_output,
             num_epochs,
             batch_size=64)->tf.data.Dataset:
    """Get post_tranform feature & create batches of data"""

    # Get post_transform feature spec
    transform_feature_spec = (
        tf_transform_output.transformed_feature_spec().copy())

    # create batches of data
    LABEL_KEY = "is_sarcastic"
    dataset = tf.data.experimental.make_batched_features_dataset(
        file_pattern=file_pattern,
        batch_size=batch_size,
        features=transform_feature_spec,
        reader=gzip_reader_fn,
        num_epochs=num_epochs,
        label_key = transformed_name(LABEL_KEY))
    return dataset

"""#### **Fungsi model_builder()**
Fungsi inilah yang bertanggung jawab dalam membuat arsitektur model. Pada latihan ini, kita menggunakan salah satu embedding layer yang tersedia dan dapat diunduh melalui TensorFlow Hub.
"""

import keras
print(keras.__version__)

import tensorflow as tf
from tensorflow.keras import layers


VOCAB_SIZE = 10000
SEQUENCE_LENGTH = 100

vectorize_layer = layers.TextVectorization(
    standardize="lower_and_strip_punctuation",
    max_tokens=VOCAB_SIZE,
    output_mode='int',
    output_sequence_length=SEQUENCE_LENGTH)

embedding_dim=16
def model_builder():
    """Build machine learning model"""
    FEATURE_KEY = "headline"
    inputs = tf.keras.Input(shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string)
    reshaped_narrative = tf.reshape(inputs, [-1])
    x = vectorize_layer(reshaped_narrative)
    x = layers.Embedding(VOCAB_SIZE, embedding_dim, name="embedding")(x)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(64, activation='relu')(x)
    x = layers.Dense(32, activation="relu")(x)
    outputs = layers.Dense(1, activation='sigmoid')(x)

    model = tf.keras.Model(inputs=inputs, outputs = outputs)

    model.compile(
        loss = 'binary_crossentropy',
        optimizer=tf.keras.optimizers.Adam(0.01),
        metrics=[tf.keras.metrics.BinaryAccuracy()]

    )

    # print(model)
    model.summary()
    return model

model = model_builder()
model.summary()

"""#### **Fungsi _get_serve_tf_examples_fn()**
Fungsi ini digunakan untuk menjalankan tahapan preprocessing data pada raw request data.
"""

def _get_serve_tf_examples_fn(model, tf_transform_output):

    model.tft_layer = tf_transform_output.transform_features_layer()

    @tf.function
    def serve_tf_examples_fn(serialized_tf_examples):
        LABEL_KEY = "is_sarcastic"

        feature_spec = tf_transform_output.raw_feature_spec()

        feature_spec.pop(LABEL_KEY)

        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)

        transformed_features = model.tft_layer(parsed_features)

        # get predictions using the transformed features
        return model(transformed_features)

    return serve_tf_examples_fn

"""#### **Fungsi run_fn()**
Fungsi yang bertanggung jawab untuk menjalankan proses training model sesuai dengan parameter training yang diberikan.
"""

from tfx.components.trainer.fn_args_utils import FnArgs
import tensorflow_transform as tft

def run_fn(fn_args: FnArgs) -> None:

    log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')

    tensorboard_callback = tf.keras.callbacks.TensorBoard(
        log_dir = log_dir, update_freq='batch'
    )

    es = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', mode='max', verbose=1, patience=10)
    mc = tf.keras.callbacks.ModelCheckpoint(fn_args.serving_model_dir, monitor='val_binary_accuracy', mode='max', verbose=1, save_best_only=True)


    # Load the transform output
    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)

    # Create batches of data
    train_set = input_fn(fn_args.train_files, tf_transform_output, 10)
    val_set = input_fn(fn_args.eval_files, tf_transform_output, 10)
    FEATURE_KEY = "headline"
    vectorize_layer.adapt(
        [j[0].numpy()[0] for j in [
            i[0][transformed_name(FEATURE_KEY)]
                for i in list(train_set)]])

    # Build the model
    model = model_builder()


    # Train the model
    model.fit(x = train_set,
            validation_data = val_set,
            callbacks = [tensorboard_callback, es, mc],
            steps_per_epoch = 1000,
            validation_steps= 1000,
            epochs=10)
    signatures = {
        'serving_default':
        _get_serve_tf_examples_fn(model, tf_transform_output).get_concrete_function(
                                    tf.TensorSpec(
                                    shape=[None],
                                    dtype=tf.string,
                                    name='examples'))
    }
    model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)

"""Seluruh fungsi di atas, akan disatukan ke dalam sebuah module file. Namun, sebelum membuat module file, kita perlu mendefinisikan nama dari module tersebut."""

TRAINER_MODULE_FILE = "sarcasm_trainer.py"

import tensorflow as tf
print(tf.keras.__version__)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {TRAINER_MODULE_FILE}
# import tensorflow as tf
# import tensorflow_transform as tft
# from tensorflow.keras import layers
# import os
# import tensorflow_hub as hub
# from tfx.components.trainer.fn_args_utils import FnArgs
# 
# LABEL_KEY = "is_sarcastic"
# FEATURE_KEY = "headline"
# 
# def transformed_name(key):
#     """Renaming transformed features"""
#     return key + "_xf"
# 
# def gzip_reader_fn(filenames):
#     """Loads compressed data"""
#     return tf.data.TFRecordDataset(filenames, compression_type='GZIP')
# 
# 
# def input_fn(file_pattern,
#              tf_transform_output,
#              num_epochs,
#              batch_size=64)->tf.data.Dataset:
#     """Get post_tranform feature & create batches of data"""
# 
#     # Get post_transform feature spec
#     transform_feature_spec = (
#         tf_transform_output.transformed_feature_spec().copy())
# 
#     # create batches of data
#     dataset = tf.data.experimental.make_batched_features_dataset(
#         file_pattern=file_pattern,
#         batch_size=batch_size,
#         features=transform_feature_spec,
#         reader=gzip_reader_fn,
#         num_epochs=num_epochs,
#         label_key = transformed_name(LABEL_KEY))
#     return dataset
# 
# # os.environ['TFHUB_CACHE_DIR'] = '/hub_chace'
# # embed = hub.KerasLayer("https://tfhub.dev/google/universal-sentence-encoder/4")
# 
# # Vocabulary size and number of words in a sequence.
# VOCAB_SIZE = 10000
# SEQUENCE_LENGTH = 100
# 
# vectorize_layer = layers.TextVectorization(
#     standardize="lower_and_strip_punctuation",
#     max_tokens=VOCAB_SIZE,
#     output_mode='int',
#     output_sequence_length=SEQUENCE_LENGTH)
# 
# 
# embedding_dim=16
# def model_builder():
#     """Build machine learning model"""
#     inputs = tf.keras.Input(shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string)
#     reshaped_narrative = tf.reshape(inputs, [-1])
#     x = vectorize_layer(reshaped_narrative)
#     x = layers.Embedding(VOCAB_SIZE, embedding_dim, name="embedding")(x)
#     x = layers.GlobalAveragePooling1D()(x)
#     x = layers.Dense(64, activation='relu')(x)
#     x = layers.Dense(32, activation="relu")(x)
#     outputs = layers.Dense(1, activation='sigmoid')(x)
# 
# 
#     model = tf.keras.Model(inputs=inputs, outputs = outputs)
# 
#     model.compile(
#         loss = 'binary_crossentropy',
#         optimizer=tf.keras.optimizers.Adam(0.01),
#         metrics=[tf.keras.metrics.BinaryAccuracy()]
# 
#     )
# 
#     # print(model)
#     model.summary()
#     return model
# 
# 
# def _get_serve_tf_examples_fn(model, tf_transform_output):
# 
#     model.tft_layer = tf_transform_output.transform_features_layer()
# 
#     @tf.function
#     def serve_tf_examples_fn(serialized_tf_examples):
# 
#         feature_spec = tf_transform_output.raw_feature_spec()
# 
#         feature_spec.pop(LABEL_KEY)
# 
#         parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)
# 
#         transformed_features = model.tft_layer(parsed_features)
# 
#         # get predictions using the transformed features
#         return model(transformed_features)
# 
#     return serve_tf_examples_fn
# 
# def run_fn(fn_args: FnArgs) -> None:
# 
#     log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')
# 
#     tensorboard_callback = tf.keras.callbacks.TensorBoard(
#         log_dir = log_dir, update_freq='batch'
#     )
# 
#     es = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', mode='max', verbose=1, patience=10)
#     mc = tf.keras.callbacks.ModelCheckpoint(fn_args.serving_model_dir, monitor='val_binary_accuracy', mode='max', verbose=1, save_best_only=True)
# 
# 
#     # Load the transform output
#     tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)
# 
#     # Create batches of data
#     train_set = input_fn(fn_args.train_files, tf_transform_output, 10)
#     val_set = input_fn(fn_args.eval_files, tf_transform_output, 10)
#     vectorize_layer.adapt(
#         [j[0].numpy()[0] for j in [
#             i[0][transformed_name(FEATURE_KEY)]
#                 for i in list(train_set)]])
# 
#     # Build the model
#     model = model_builder()
# 
# 
#     # Train the model
#     model.fit(x = train_set,
#             validation_data = val_set,
#             callbacks = [tensorboard_callback, es, mc],
#             steps_per_epoch = 1000,
#             validation_steps= 1000,
#             epochs=10)
#     signatures = {
#         'serving_default':
#         _get_serve_tf_examples_fn(model, tf_transform_output).get_concrete_function(
#                                     tf.TensorSpec(
#                                     shape=[None],
#                                     dtype=tf.string,
#                                     name='examples'))
#     }

from tfx.proto import trainer_pb2

trainer  = Trainer(
    module_file=os.path.abspath(TRAINER_MODULE_FILE),
    examples = transform.outputs['transformed_examples'],
    transform_graph=transform.outputs['transform_graph'],
    schema=schema_gen.outputs['schema'],
    train_args=trainer_pb2.TrainArgs(splits=['train']),
    eval_args=trainer_pb2.EvalArgs(splits=['eval'])
)
interactive_context.run(trainer)

"""## **Analisis dan Validasi Model**

### **Komponen Resolver**
Untuk melakukan analisis dan validasi model, kita perlu menyediakan sebuah baseline model. Hal ini sangat penting terutama ketika kita memiliki lebih dari satu versi model dan ingin membandingkan dua buah versi model yang berbeda. Untuk melakukannya, kita bisa memanfaatkan komponen Resolver().
"""

from tfx.dsl.components.common.resolver import Resolver
from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import LatestBlessedModelStrategy
from tfx.types import Channel
from tfx.types.standard_artifacts import Model, ModelBlessing

model_resolver = Resolver(
    strategy_class= LatestBlessedModelStrategy,
    model = Channel(type=Model),
    model_blessing = Channel(type=ModelBlessing)
).with_id('Latest_blessed_model_resolver')

interactive_context.run(model_resolver)

"""### **Komponen Evaluator**
Setelah mendefinisikan komponen Resolver, tahap selanjutnya adalah membuat beberapa konfigurasi untuk mengevaluasi model. Konfigurasi ini dibuat menggunakan library TFMA seperti berikut.
"""

import tensorflow_model_analysis as tfma

eval_config = tfma.EvalConfig(
    model_specs=[tfma.ModelSpec(label_key='is_sarcastic')],
    slicing_specs=[tfma.SlicingSpec()],
    metrics_specs=[
        tfma.MetricsSpec(metrics=[

            tfma.MetricConfig(class_name='ExampleCount'),
            tfma.MetricConfig(class_name='AUC'),
            tfma.MetricConfig(class_name='FalsePositives'),
            tfma.MetricConfig(class_name='TruePositives'),
            tfma.MetricConfig(class_name='FalseNegatives'),
            tfma.MetricConfig(class_name='TrueNegatives'),
            tfma.MetricConfig(class_name='BinaryAccuracy',
                threshold=tfma.MetricThreshold(
                    value_threshold=tfma.GenericValueThreshold(
                        lower_bound={'value':0.5}),
                    change_threshold=tfma.GenericChangeThreshold(
                        direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                        absolute={'value':0.0001})
                    )
            )
        ])
    ]

)

"""Setelah membuat konfigurasi yang akan digunakan untuk mengevaluasi model, tahap berikutnya adalah mendefinisikan komponen Evaluator. Komponen ini akan menerima beberapa input seperti berikut.
examples untuk menerima dataset dari komponen ExampleGen.

model untuk menerima model yang dihasilkan dari komponen Trainer.

baseline_model untuk menampung baseline model yang disediakan oleh komponen Resolver.

eval_config untuk menerima konfigurasi untuk mengevaluasi model
"""

from tfx.components import Evaluator
evaluator = Evaluator(
    examples=example_gen.outputs['examples'],
    model=trainer.outputs['model'],
    baseline_model=model_resolver.outputs['model'],
    eval_config=eval_config)

interactive_context.run(evaluator)

"""Hasil evaluasi dari komponen ini dapat Anda visualisasikan menggunakan library TFMA seperti contoh kode di bawah ini."""

# Visualize the evaluation results
eval_result = evaluator.outputs['evaluation'].get()[0].uri
tfma_result = tfma.load_eval_result(eval_result)
tfma.view.render_slicing_metrics(tfma_result)
tfma.addons.fairness.view.widget_view.render_fairness_indicator(
    tfma_result
)

"""### Komponen Tuner

Pada latihan sebelumnya, kita telah melakukan tahap pengembangan model menggunakan komponen Trainer. Namun, pada latihan tersebut, kita hanya melakukan proses model training dan belum mencakup proses model tuning.

Pada TFX pipeline, proses model tuning akan dikerjakan oleh komponen Tuner. Ia akan melakukan proses model tuning secara otomatis dengan bantuan keras tuner. Seperti yang kita bahas pada modul sebelumnya, komponen ini menerima beberapa input seperti berikut.

Data schema yang diperoleh dari tahap data validation.
Transformed data yang diperoleh dari tahap data preprocessing.
Parameter tuning dan training.
Sebuah module file yang berisi tuner function.
"""

def tuner_fn(fn_args):
  """Build the tuner using the KerasTuner API.
  Args:
    fn_args: Holds args used to tune models as name/value pairs.

  Returns:
    A namedtuple contains the following:
      - tuner: A BaseTuner that will be used for tuning.
      - fit_kwargs: Args to pass to tuner's run_trial function for fitting the
                    model , e.g., the training and validation dataset. Required
                    args depend on the above tuner's implementation.
  """
  # Memuat training dan validation dataset yang telah di-preprocessing
  tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)
  train_set = input_fn(fn_args.train_files[0], tf_transform_output)
  val_set = input_fn(fn_args.eval_files[0], tf_transform_output)

  # Mendefinisikan strategi hyperparameter tuning
  tuner = kt.Hyperband(model_builder,
                     objective='val_accuracy',
                     max_epochs=10,
                     factor=3,
                     directory=fn_args.working_dir,
                     project_name='kt_hyperband')

  return TunerFnResult(
      tuner=tuner,
      fit_kwargs={
          "callbacks":[stop_early],
          'x': train_set,
          'validation_data': val_set,
          'steps_per_epoch': fn_args.train_steps,
          'validation_steps': fn_args.eval_steps
      }
  )

from tfx.components import Tuner
from tfx.proto import trainer_pb2

tuner = Tuner(
    module_file=tuner_module_file,
    examples=transform.outputs['transformed_examples'],
    transform_graph=transform.outputs['transform_graph'],
    schema=schema_gen.outputs['schema'],
    train_args=trainer_pb2.TrainArgs(splits=['train'], num_steps=500),
    eval_args=trainer_pb2.EvalArgs(splits=['eval'], num_steps=100)
    )

interactive_context(tuner)